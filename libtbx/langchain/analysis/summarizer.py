"""
Log file summarization using map-reduce approach.

This module handles:
- Chunking log files for processing
- Map phase: summarize each chunk
- Reduce phase: combine chunk summaries into final report

Usage:
    from libtbx.langchain.analysis import summarize_log_text

    result = await summarize_log_text(log_text, llm, provider='google')
    if result.error:
        print(f"Error: {result.error}")
    else:
        print(result.log_summary)
"""
from __future__ import absolute_import, division, print_function

import asyncio
from typing import List, Iterable

from langchain_core.documents import Document
from langchain_core.prompts import PromptTemplate
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_text_splitters import RecursiveCharacterTextSplitter

from libtbx import group_args


# =============================================================================
# Prompt Templates
# =============================================================================

def get_log_map_prompt() -> PromptTemplate:
    """Returns the prompt for summarizing a single chunk of a log file."""
    template = """
    You are a highly skilled data extraction bot. Your task is to scan a chunk of a Phenix log file and pull out only the most critical information.

    **Instructions:**
    1.  **Identify Key Steps:** Look for lines that indicate the start or end of a major computational step.
    2.  **Extract File Names:** List any input/output files mentioned. Ignore .dat files. Capture 'overall_best' files.
    3.  **Capture Metrics:** Record specific numbers (CC, resolution, R-values, etc.). Report final/refined values.
    4.  **Identify Data Type:** Note if data is X-ray or CryoEM.
    5.  **Note Warnings/Errors:** Capture critical warnings.
    6.  **Note Space Group:** Record space group and any recommendations.
    7.  **Identify Program:** Name the program if clear.
    8.  **Be Concise:** Brief, bulleted list.

    Log Chunk:
    "{text}"

    Concise, structured summary of this chunk:
    """
    return PromptTemplate.from_template(template)


def get_log_combine_prompt() -> PromptTemplate:
    """Returns the prompt for synthesizing log chunk summaries into a final report."""
    template = (
        "You are an expert research scientist. Synthesize the following summaries "
        "from a long log file into a structured report. Your report "
        "must contain the key information requested below.\n\n"
        "Final Report Requirements:\n"
        "1. **Input Files:** List key input files and data type (X-ray or cryo-EM).\n"
        "2. **Program Name:** The name of the Phenix program that was run.\n"
        "3. **Key Steps:** A high-level, bulleted list of the main steps carried out.\n"
        "4. **Key Metrics:** A concise table of the *final* key metrics (e.g., CC, R-values, resolution).\n"
        "5. **Key Space Group:** Specify the space group used and any recommendations.\n"
        "6. **Warnings/Errors:** A list of any critical warnings or errors.\n"
        "7. **Key Output Files:** A comma-separated list of the most important output filenames generated by this run (e.g. model.pdb, data.mtz).\n\n"

        "**IMPORTANT:** Be structured and clear. Do NOT provide 'Detailed descriptions' or 'Full reproductions' of tables. "
        "Focus on the most critical, final information. "
        "Do not add conversational text or offer help.\n\n"

        "Now, synthesize the following summaries:\n"
        "{context}"
    )
    return PromptTemplate(template=template, input_variables=["context"])


# =============================================================================
# Helper Functions
# =============================================================================

def get_chunk_size(provider: str = 'google'):
    """
    Returns appropriate chunk size based on provider.

    Args:
        provider: 'google' or 'openai'

    Returns:
        tuple: (chunk_size, chunk_overlap)
    """
    provider = provider.lower()
    if provider == "openai":
        chunk_size = 100000
        chunk_overlap = 10000
        print("Using smaller chunk size for OpenAI.")
    elif provider == "google":
        chunk_size = 750000
        chunk_overlap = 50000
        print("Using larger chunk size for Google Gemini.")
    else:
        chunk_size = 100000
        chunk_overlap = 10000
        print(f"Warning: Unknown provider '{provider}'. Defaulting to smaller chunk size.")
    return chunk_size, chunk_overlap


def _iter_batches(seq: List[Document], size: int) -> Iterable[List[Document]]:
    """Yield successive batches from sequence."""
    for i in range(0, len(seq), size):
        yield seq[i:i+size]


def _custom_log_chunker(log_text: str, provider: str = "google") -> List[Document]:
    """
    Chunks a log text with a special rule for the 'Files are in the directory' section.

    Args:
        log_text: The full log file text
        provider: 'google' or 'openai' (affects chunk size)

    Returns:
        List of Document objects, each containing a chunk
    """
    chunk_size, chunk_overlap = get_chunk_size(provider)

    trigger_phrase = "Files are in the directory"
    end_phrase = "Citations"

    final_chunks = []
    start_index = log_text.lower().find(trigger_phrase.lower())

    if start_index == -1:
        documents = [Document(page_content=log_text)]
        standard_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size, chunk_overlap=chunk_overlap
        )
        return standard_splitter.split_documents(documents)

    before_text = log_text[:start_index].strip()
    if before_text:
        before_doc = [Document(page_content=before_text)]
        standard_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size, chunk_overlap=chunk_overlap
        )
        final_chunks.extend(standard_splitter.split_documents(before_doc))

    special_section_text = log_text[start_index:]
    end_index = special_section_text.find(end_phrase)

    if end_index != -1:
        special_chunk_content = special_section_text[:end_index].strip()
    else:
        special_chunk_content = special_section_text.strip()

    if special_chunk_content:
        final_chunks.append(Document(page_content=special_chunk_content))

    return final_chunks


# =============================================================================
# Main Summarization Function
# =============================================================================

async def summarize_log_text(
    text: str,
    llm,
    timeout: int = 120,
    batch_size: int = 3,
    pause_between_batches: int = 1,
    use_throttling: bool = True,
    provider: str = 'google',
):
    """
    Performs a map-reduce summarization with batching to respect API rate limits.

    Args:
        text: The log file content to summarize
        llm: Language model to use for summarization
        timeout: Timeout in seconds for each batch
        batch_size: Number of chunks to process in parallel
        pause_between_batches: Seconds to pause between batches
        use_throttling: Whether to pause between batches
        provider: 'google' or 'openai'

    Returns:
        group_args with:
            - group_args_type: 'log_summary'
            - log_summary: The final summary text (or None if failed)
            - error: Error message (or None if successful)

    Example:
        result = await summarize_log_text(log_text, llm)
        if result.error:
            print(f"Failed: {result.error}")
        else:
            print(result.log_summary)
    """
    docs = _custom_log_chunker(text, provider=provider)
    if not docs:
        return group_args(
            group_args_type='log_summary',
            log_summary=None,
            error="Log file produced no content to summarize."
        )

    map_prompt = get_log_map_prompt()
    map_chain = map_prompt | llm

    all_intermediate_summaries = []

    num_batches = (len(docs) + batch_size - 1) // batch_size
    print(f"Summarizing {len(docs)} chunks in {num_batches} batches to respect rate limits...")

    for i, batch in enumerate(_iter_batches(docs, batch_size)):
        print(f"  - Processing batch {i + 1} of {num_batches}...")

        tasks = [map_chain.ainvoke({"text": doc.page_content}) for doc in batch]

        try:
            map_results = await asyncio.wait_for(asyncio.gather(*tasks), timeout=timeout)
            all_intermediate_summaries.extend([result.content for result in map_results])
        except asyncio.TimeoutError:
            print(f"Batch {i + 1} timed out after {timeout} seconds. Proceeding with partial results.")
            continue

        if use_throttling and (i < num_batches - 1):
            print(f"  - Pausing {pause_between_batches} seconds ...")
            await asyncio.sleep(pause_between_batches)

    if not all_intermediate_summaries:
        return group_args(
            group_args_type='log_summary',
            log_summary=None,
            error="Log summarization failed to produce any results."
        )

    summary_docs = [Document(page_content=s) for s in all_intermediate_summaries]

    combine_prompt = get_log_combine_prompt()
    reduce_chain = create_stuff_documents_chain(llm, combine_prompt)
    final_output = reduce_chain.invoke({"context": summary_docs})

    return group_args(
        group_args_type='log_summary',
        log_summary=final_output,
        error=None
    )
