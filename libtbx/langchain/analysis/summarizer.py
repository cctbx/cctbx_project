"""
Log file summarization using map-reduce approach.

This module handles:
- Chunking log files for processing
- Map phase: summarize each chunk
- Reduce phase: combine chunk summaries into final report

Usage:
    from libtbx.langchain.analysis import summarize_log_text

    result = await summarize_log_text(log_text, llm, provider='google')
    if result.error:
        print(f"Error: {result.error}")
    else:
        print(result.log_summary)
"""
from __future__ import absolute_import, division, print_function

import asyncio
from typing import List, Iterable
import os

from langchain_core.documents import Document
from langchain_core.prompts import PromptTemplate
from langchain_classic.chains.combine_documents import create_stuff_documents_chain
from langchain_text_splitters import RecursiveCharacterTextSplitter

from libtbx import group_args

# =============================================================================
# Debug flag - set to True to enable verbose output
# =============================================================================
DEBUG_SUMMARIZER = True #os.getenv("DEBUG_SUMMARIZER", "false").lower() == "true"

def debug_print(msg):
    """Print debug message if DEBUG_SUMMARIZER is enabled."""
    if DEBUG_SUMMARIZER:
        print(f"[SUMMARIZER DEBUG] {msg}")


# =============================================================================
# Prompt Templates
# =============================================================================

def get_log_map_prompt() -> PromptTemplate:
    """Returns the prompt for summarizing a single chunk of a log file."""
    template = """
You are a data extraction bot. Extract the following information from this Phenix log file chunk.

**CRITICAL: Analyze the ENTIRE log text below, not just the last few lines. Do NOT just copy any "FINAL QUALITY METRICS" section - extract information from throughout the log.**

YOU MUST EXTRACT ALL OF THESE IF PRESENT:

1. PROGRAM NAME: Look for "PHENIX:" or "phenix." in the first few lines
2. INPUT FILES: Look for file paths ending in .mtz, .pdb, .fa, .seq
3. OUTPUT FILES: Look for "written to" or "HKLOUT" or output file names like PHASER.1.pdb
4. SPACE GROUP: Look for "SPACEGROUP" or "Space group" followed by something like "P 31 2 1"
5. DATA TYPE: If .mtz file present, it's X-ray data
6. METRICS - Extract ALL numbers for:
   - TFZ (Translation Function Z-score) - VERY IMPORTANT
   - RFZ (Rotation Function Z-score)
   - LLG (Log-Likelihood Gain)
   - eLLG (expected LLG)
   - CC (Correlation Coefficient)
   - R-values (R-work, R-free)
   - Resolution
7. WARNINGS/ERRORS: Lines starting with "Warning:" or "Sorry:" or "Error:"

Format your response as a bulleted list with these exact headers.

Log Chunk:
"{text}"

Extracted information (analyze the ENTIRE log above, use exact headers):
"""
    return PromptTemplate.from_template(template)

def get_log_combine_prompt() -> PromptTemplate:
    """Returns the prompt for synthesizing log chunk summaries into a final report."""
    template = (
        "You are an expert research scientist. Synthesize the following summaries "
        "from a long log file into a structured report. Your report "
        "must contain the key information requested below.\n\n"
        "Final Report Requirements:\n"
        "1. **Input Files:** List key input files and data type (X-ray or cryo-EM).\n"
        "2. **Program Name:** The name of the Phenix program that was run.\n"
        "3. **Key Steps:** A high-level, bulleted list of the main steps carried out.\n"
        "4. **Key Metrics:** A concise table of the *final* key metrics (e.g., CC, R-values, resolution, TFZ, LLG).\n"
        "5. **Key Space Group:** Specify the space group used and any recommendations.\n"
        "6. **Warnings/Errors:** A list of any critical warnings or errors.\n"
        "7. **Key Output Files:** A comma-separated list of the most important output filenames generated by this run (e.g. PHASER.1.pdb, refined_001.mtz). Use ACTUAL filenames from the summaries, not generic placeholders.\n\n"

        "**IMPORTANT:** Be structured and clear. Do NOT provide 'Detailed descriptions' or 'Full reproductions' of tables. "
        "Focus on the most critical, final information. "
        "Do not add conversational text or offer help.\n\n"

        "Now, synthesize the following summaries:\n"
        "{context}"
    )
    return PromptTemplate(template=template, input_variables=["context"])



# =============================================================================
# Helper Functions
# =============================================================================

def get_chunk_size(provider: str = None):
    """
    Returns appropriate chunk size based on provider.

    Args:
        provider: 'google' or 'openai' or 'ollama'

    Returns:
        tuple: (chunk_size, chunk_overlap)
    """
    if provider is None:
        provider = os.getenv("LLM_PROVIDER", "ollama")

    provider = provider.lower()
    if provider == "openai":
        chunk_size = 100000
        chunk_overlap = 10000
        debug_print(f"OpenAI: chunk_size={chunk_size}, overlap={chunk_overlap}")
    elif provider == "google":
        chunk_size = 750000
        chunk_overlap = 50000
        debug_print(f"Google: chunk_size={chunk_size}, overlap={chunk_overlap}")
    elif provider == "ollama":
        chunk_size = 80000
        chunk_overlap = 8000
        debug_print(f"Ollama: chunk_size={chunk_size}, overlap={chunk_overlap}")
    else:
        chunk_size = 100000
        chunk_overlap = 10000
        debug_print(f"Unknown provider '{provider}': using default chunk_size={chunk_size}")
    return chunk_size, chunk_overlap


def _iter_batches(seq: List[Document], size: int) -> Iterable[List[Document]]:
    """Yield successive batches from sequence."""
    for i in range(0, len(seq), size):
        yield seq[i:i+size]


def _custom_log_chunker(log_text: str, provider: str = "google") -> List[Document]:
    """
    Chunks a log text with a special rule for the 'Files are in the directory' section.

    Args:
        log_text: The full log file text
        provider: 'google' or 'openai' or 'ollama'

    Returns:
        List of Document objects, each containing a chunk
    """
    chunk_size, chunk_overlap = get_chunk_size(provider)

    debug_print(f"Input log_text length: {len(log_text)} chars")

    trigger_phrase = "Files are in the directory"
    end_phrase = "Citations"

    final_chunks = []
    start_index = log_text.lower().find(trigger_phrase.lower())

    if start_index == -1:
        debug_print("No 'Files are in the directory' section found - using standard splitting")
        documents = [Document(page_content=log_text)]
        standard_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size, chunk_overlap=chunk_overlap
        )
        chunks = standard_splitter.split_documents(documents)
        debug_print(f"Standard splitting produced {len(chunks)} chunks")
        for i, chunk in enumerate(chunks):
            debug_print(f"  Chunk {i+1}: {len(chunk.page_content)} chars, starts with: {chunk.page_content[:100]!r}...")
        return chunks

    debug_print(f"Found 'Files are in the directory' at position {start_index}")

    before_text = log_text[:start_index].strip()
    if before_text:
        debug_print(f"Before section: {len(before_text)} chars")
        before_doc = [Document(page_content=before_text)]
        standard_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size, chunk_overlap=chunk_overlap
        )
        before_chunks = standard_splitter.split_documents(before_doc)
        debug_print(f"Before section split into {len(before_chunks)} chunks")
        final_chunks.extend(before_chunks)

    special_section_text = log_text[start_index:]
    end_index = special_section_text.find(end_phrase)

    if end_index != -1:
        special_chunk_content = special_section_text[:end_index].strip()
        debug_print(f"Special section (before '{end_phrase}'): {len(special_chunk_content)} chars")
    else:
        special_chunk_content = special_section_text.strip()
        debug_print(f"Special section (to end): {len(special_chunk_content)} chars")

    if special_chunk_content:
        final_chunks.append(Document(page_content=special_chunk_content))

    debug_print(f"Total chunks after custom splitting: {len(final_chunks)}")
    for i, chunk in enumerate(final_chunks):
        debug_print(f"  Chunk {i+1}: {len(chunk.page_content)} chars")
        debug_print(f"    Starts: {chunk.page_content[:80]!r}...")
        debug_print(f"    Ends: ...{chunk.page_content[-80:]!r}")

    return final_chunks


# =============================================================================
# Main Summarization Function
# =============================================================================

async def summarize_log_text(
    text: str,
    llm,
    timeout: int = 120,
    batch_size: int = 3,
    pause_between_batches: int = 1,
    use_throttling: bool = True,
    provider: str = None,
):
    """
    Performs a map-reduce summarization with batching to respect API rate limits.

    Args:
        text: The log file content to summarize
        llm: Language model to use for summarization
        timeout: Timeout in seconds for each batch
        batch_size: Number of chunks to process in parallel
        pause_between_batches: Seconds to pause between batches
        use_throttling: Whether to pause between batches
        provider: 'google' or 'openai' or 'ollama'

    Returns:
        group_args with:
            - group_args_type: 'log_summary'
            - log_summary: The final summary text (or None if failed)
            - error: Error message (or None if successful)
    """
    if provider is None:
        provider = os.getenv("LLM_PROVIDER", "ollama")

    debug_print("=" * 60)
    debug_print(f"SUMMARIZE_LOG_TEXT STARTING")
    debug_print(f"  Provider: {provider}")
    debug_print(f"  LLM model: {getattr(llm, 'model', getattr(llm, 'model_name', 'unknown'))}")
    debug_print(f"  Input text length: {len(text)} chars")
    debug_print(f"  Timeout: {timeout}s, Batch size: {batch_size}")
    debug_print("=" * 60)

    docs = _custom_log_chunker(text, provider=provider)
    if not docs:
        debug_print("ERROR: No chunks produced!")
        return group_args(
            group_args_type='log_summary',
            log_summary=None,
            error="Log file produced no content to summarize."
        )

    map_prompt = get_log_map_prompt()
    map_chain = map_prompt | llm

    debug_print(f"\n--- MAP PHASE: Summarizing {len(docs)} chunks ---")

    all_intermediate_summaries = []

    num_batches = (len(docs) + batch_size - 1) // batch_size
    print(f"Summarizing {len(docs)} chunks in {num_batches} batches to respect rate limits...")

    for i, batch in enumerate(_iter_batches(docs, batch_size)):
        print(f"  - Processing batch {i + 1} of {num_batches}...")
        debug_print(f"\n  BATCH {i+1}/{num_batches}: {len(batch)} chunks")

        for j, doc in enumerate(batch):
            debug_print(f"    Chunk {j+1} input ({len(doc.page_content)} chars):")
            debug_print(f"      First 200 chars: {doc.page_content[:200]!r}")
            debug_print(f"      Last 200 chars: ...{doc.page_content[-200:]!r}")

        tasks = [map_chain.ainvoke({"text": doc.page_content}) for doc in batch]

        try:
            map_results = await asyncio.wait_for(asyncio.gather(*tasks), timeout=timeout)
            
            for j, result in enumerate(map_results):
                summary_text = result.content
                all_intermediate_summaries.append(summary_text)
                debug_print(f"\n    Chunk {j+1} MAP OUTPUT ({len(summary_text)} chars):")
                debug_print("-" * 40)
                # Print full intermediate summary for debugging
                for line in summary_text.split('\n')[:30]:  # First 30 lines
                    debug_print(f"      {line}")
                if summary_text.count('\n') > 30:
                    debug_print(f"      ... ({summary_text.count(chr(10)) - 30} more lines)")
                debug_print("-" * 40)

        except asyncio.TimeoutError:
            print(f"Batch {i + 1} timed out after {timeout} seconds. Proceeding with partial results.")
            debug_print(f"  BATCH {i+1} TIMEOUT!")
            continue

        if use_throttling and (i < num_batches - 1):
            print(f"  - Pausing {pause_between_batches} seconds ...")
            await asyncio.sleep(pause_between_batches)

    if not all_intermediate_summaries:
        debug_print("ERROR: No intermediate summaries produced!")
        return group_args(
            group_args_type='log_summary',
            log_summary=None,
            error="Log summarization failed to produce any results."
        )

    debug_print(f"\n--- REDUCE PHASE: Combining {len(all_intermediate_summaries)} summaries ---")

    summary_docs = [Document(page_content=s) for s in all_intermediate_summaries]

    # Debug: show what's being combined
    debug_print("Intermediate summaries being combined:")
    for i, doc in enumerate(summary_docs):
        debug_print(f"\n  === INTERMEDIATE SUMMARY {i+1} ({len(doc.page_content)} chars) ===")
        debug_print(doc.page_content)
        debug_print("  === END ===\n")

    combine_prompt = get_log_combine_prompt()
    
    # Debug: show the combine prompt
    debug_print("COMBINE PROMPT TEMPLATE:")
    debug_print(combine_prompt.template[:500] + "...")

    reduce_chain = create_stuff_documents_chain(llm, combine_prompt)
    final_output = reduce_chain.invoke({"context": summary_docs})

    debug_print(f"\n--- FINAL OUTPUT ({len(final_output)} chars) ---")
    debug_print("=" * 60)
    debug_print(final_output)
    debug_print("=" * 60)

    return group_args(
        group_args_type='log_summary',
        log_summary=final_output,
        error=None
    )

