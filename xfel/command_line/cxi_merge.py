# -*- mode: python; coding: utf-8; indent-tabs-mode: nil; python-indent: 2 -*-
#
# LIBTBX_SET_DISPATCHER_NAME cxi.merge
# LIBTBX_SET_DISPATCHER_NAME xfel.merge
#
# $Id$

from __future__ import division

from rstbx.dials_core.integration_core import show_observations
import iotbx.phil
from iotbx import data_plots
from cctbx.array_family import flex
from cctbx import miller
from cctbx.crystal import symmetry
from cctbx.sgtbx.bravais_types import bravais_lattice
from cctbx import uctbx
from libtbx.str_utils import format_value
from libtbx.utils import Usage, Sorry, multi_out
from libtbx import easy_pickle
from libtbx import adopt_init_args, group_args, Auto
from cStringIO import StringIO
import os
import math
import time
import sys
from scitbx import matrix
op = os.path

from xfel.cxi.merging_database import mysql_master_phil
master_phil="""
data = None
  .type = path
  .multiple = True
  .help = Directory containing integrated data in pickle format.  Repeat to \
    specify additional directories.
data_subset = 0
  .type = int
  .help = 0: use all data / 1: use odd-numbered frames / 2: use even-numbered frames
model = None
  .type = str
  .help = PDB filename containing atomic coordinates & isomorphous cryst1 record
model_reindex_op = h,k,l
  .type = str
  .help = Kludge for cases with an indexing ambiguity, need to be able to adjust scaling model
data_reindex_op = h,k,l
  .type = str
  .help = Reindex, e.g. to change C-axis of an orthorhombic cell to align Bravais lattice from indexing with actual space group
target_unit_cell = None
  .type = unit_cell
  .help = leave as None, program uses the model PDB file cryst1 record
target_space_group = None
  .type = space_group
  .help = leave  as None, program uses the model PDB file cryst1 record
set_average_unit_cell = False
  .type = bool
rescale_with_average_cell = False
  .type = bool
d_min = None
  .type = float
  .help = limiting resolution for scaling and merging
d_max = None
  .type = float
  .help = limiting resolution for scaling and merging.  Implementation currently affects only the CCiso cal
k_sol = 0.35
  .type = float
  .help = bulk solvent scale factor - approximate mean value in PDB \
    (according to Pavel)
b_sol = 46.00
  .type = float
  .help = bulk solvent B-factor - approximate mean value in PDB \
    (according to Pavel)
merge_anomalous = False
  .type = bool
  .help = Merge anomalous contributors
include_bulk_solvent = True
  .type = bool
wavelength = None
  .type = float
pixel_size = None
  .type = float
  .help = Detector-specific parameter for pixel size in mm
elements = None
  .type = str
  .multiple = True
significance_filter {
  apply = True
    .type = bool
    .help = Apply a sigma cutoff (on unmerged data) to limit resolution from each diffraction pattern
  n_bins = 12
    .type = int (value_min=2)
    .help = Initial target number of resolution bins for sigma cutoff calculation
  min_ct = 10
    .type = int
    .help = Decrease number of resolution bins to require mean bin population >= min_ct
  max_ct = 50
    .type = int
    .help = Increase number of resolution bins to require mean bin population <= max_ct
  sigma = 0.5
    .type = float
    .help = Remove highest resolution bins such that all accepted bins have <I/sigma> >= sigma
}
min_corr = 0.1
  .type = float
  .help = Correlation cutoff for rejecting individual frames.
  .help = This filter is not applied if model==None.
unit_cell_length_tolerance = 0.1
  .type = float
  .help = Fractional change in unit cell dimensions allowed (versus target \
    cell).
unit_cell_angle_tolerance = 2.
  .type = float
nproc = None
  .type = int
raw_data {
  sdfac_auto = False
    .type = bool
    .help = apply sdfac to each-image data assuming negative intensities are normally distributed noise
}
output {
  n_bins = 10
    .type = int
    .help = Number of resolution bins in statistics table
  prefix = iobs
    .type = str
    .help = Prefix for all output file names
  title = None
    .type = str
    .help = Title for run - will appear in MTZ file header
}
merging {
  refine_G_Imodel = False
    .type = bool
    .help = "Refine per-frame scaling factors and model intensities
             after initial merging"
  reverse_lookup = None
    .type = str
    .help = filename, pickle format, generated by the cxi.brehm_diederichs program.  Contains a
    .help = (key,value) dictionary where key is the filename of the integrated data pickle file (supplied
    .help = with the data phil parameter and value is the h,k,l reindexing operator that resolves the
    .help = indexing ambiguity.
}
scaling {
  mtz_file = None
    .type = str
    .help = for Riso/ CCiso, the reference structure factors, must have data type F
    .help = a fake file is written out to this file name if model is None
  mtz_column_F = fobs
    .type = str
    .help = for Riso/ CCiso, the column name containing reference structure factors
  log_cutoff = None
    .type = float
    .help = for CC calculation, log(intensity) cutoff, ignore values less than this
  show_plots = False
    .type = bool
  enable = True
    .type = bool
    .help = enable the mark0 algorithm, otherwise individual-image scale factors are set to 1.0
    .expert_level = 3
  algorithm = *mark0 mark1
    .type = choice
    .help = "mark0: original per-image scaling by reference to
             isomorphous PDB model"
    .help = "mark1: no scaling, just averaging (i.e. Monte Carlo
             algorithm).  Individual image scale factors are set to 1."
  simulation = None
    .type = str
    .help = To test scaling, use simulated data from the model instead of the actual observations
    .help = String value governs how the sim data are calculated
  simulation_data = None
    .type = floats
    .help = Extra parameters for the simulation, exact meaning depends on calculation method
  report_ML = False
    .type = bool
    .help = Report statistics on per-frame attributes modeled by max-likelihood fit (expert only)
}
postrefinement {
  enable = False
    .type = bool
    .help = enable the preliminary postrefinement algorithm (monochromatic)
    .expert_level = 3
  algorithm = *rs eta_deff
    .type = choice
    .help = rs only, eta_deff protocol 7
    .expert_level = 3
}
plot_single_index_histograms = False
  .type = bool
""" + mysql_master_phil

def get_observations (data_dirs,data_subset):
  print "Step 1.  Get a list of all files"
  file_names = []
  for dir_name in data_dirs :
    if not os.path.isdir(dir_name):
      continue
    for file_name in os.listdir(dir_name):
      if (file_name.endswith("_00000.pickle")):
        if data_subset==0 or \
          (data_subset==1 and (int(os.path.basename(file_name).split("_00000.pickle")[0][-1])%2==1)) or \
          (data_subset==2 and (int(os.path.basename(file_name).split("_00000.pickle")[0][-1])%2==0)):
          file_names.append(os.path.join(dir_name, file_name))
      elif (file_name.endswith(".pickle")):
        if data_subset==0 or \
          (data_subset==1 and (int(os.path.basename(file_name).split(".pickle")[0][-1])%2==1)) or \
          (data_subset==2 and (int(os.path.basename(file_name).split(".pickle")[0][-1])%2==0)):
          file_names.append(os.path.join(dir_name, file_name))
  print "Number of pickle files found:", len(file_names)
  print
  return file_names

class WrongBravaisError (Exception) :
  pass

class OutlierCellError (Exception) :
  pass

def load_result (file_name,
                 ref_bravais_type,
                 reference_cell,
                 params,
                 reindex_op,
                 out) :
  # If @p file_name cannot be read, the load_result() function returns
  # @c None.

  print "Step 2.  Load pickle file into dictionary obj and filter on lattice & cell with",reindex_op
  print file_name
  """
  Take a pickle file, confirm that it contains the appropriate data, and
  check the lattice type and unit cell against the reference settings - if
  rejected, raises an exception (for tracking statistics).
  """
  # Ignore corrupted pickle files.
  try:
    obj = easy_pickle.load(file_name=file_name)
  except Exception:
    return None
  if (not obj.has_key("observations")) :
    return None

  if reindex_op == "h,k,l":
    pass
    #raise WrongBravaisError("Skipping file with h,k,l")
  else:
    for idx in xrange(len(obj["observations"])):
      obj["observations"][idx] = obj["observations"][idx].change_basis(reindex_op)
    pass
    #raise WrongBravaisError("Skipping file with alternate indexing %s"%reindex_op)

  result_array = obj["observations"][0]
  unit_cell = result_array.unit_cell()
  sg_info = result_array.space_group_info()
  print >> out, ""
  print >> out, "-" * 80
  print >> out, file_name
  print >> out, sg_info
  print >> out, unit_cell

  #Check for pixel size (at this point we are assuming we have square pixels, all experiments described in one
  #refined_experiments.json file use the same detector, and all panels on the detector have the same pixel size)

  if params.pixel_size is not None:
    pixel_size = params.pixel_size
  elif "pixel_size" in obj:
    pixel_size = obj["pixel_size"]
  else:
    raise Sorry("Cannot find pixel size. Specify appropriate pixel size in mm for your detector in phil file.")

  #Calculate displacements based on pixel size
  assert obj['mapped_predictions'][0].size() == obj["observations"][0].size()
  mm_predictions = pixel_size*(obj['mapped_predictions'][0])
  mm_displacements = flex.vec3_double()
  cos_two_polar_angle = flex.double()
  for pred in mm_predictions:
    mm_displacements.append((pred[0]-obj["xbeam"],pred[1]-obj["ybeam"],0.0))
    cos_two_polar_angle.append( math.cos( 2. * math.atan2(pred[1]-obj["ybeam"],pred[0]-obj["xbeam"]) ) )
  obj["cos_two_polar_angle"] = cos_two_polar_angle
  #then convert to polar angle and compute polarization correction

  if (not bravais_lattice(sg_info.type().number()) == ref_bravais_type) :
    raise WrongBravaisError("Skipping cell in different Bravais type (%s)" %
      str(sg_info))
  if (not unit_cell.is_similar_to(
      other=reference_cell,
      relative_length_tolerance=params.unit_cell_length_tolerance,
      absolute_angle_tolerance=params.unit_cell_angle_tolerance)) :
    raise OutlierCellError(
      "Skipping cell with outlier dimensions (%g %g %g %g %g %g" %
      unit_cell.parameters())
  # Illustrate how a unit cell filter would be implemented.
  #ucparams = unit_cell.parameters()
  #if not (130.21 < ucparams[2] < 130.61) or not (92.84 < ucparams[0] < 93.24):
  #  print "DOES NOT PASS ERSATZ UNIT CELL FILTER"
  #  return None
  print >> out, "Integrated data:"
  result_array.show_summary(f=out, prefix="  ")
  # XXX don't force reference setting here, it will be done later, after the
  # original unit cell is recorded
  return obj

class intensity_data (object) :
  """
  Container for scaled intensity data.
  """
  def __init__ (self, n_refl) :
    self.n_refl = n_refl
    self.initialize()

  def initialize (self) :
    self.ISIGI        = {}
    self.completeness = flex.int(self.n_refl, 0)
    self.summed_N     = flex.int(self.n_refl, 0)
    self.summed_weight= flex.double(self.n_refl, 0.)
    self.summed_wt_I  = flex.double(self.n_refl, 0.)

class frame_data (intensity_data) :
  """
  Intensity data for a single frame.
  """
  def __init__ (self, n_refl, file_name) :
    intensity_data.__init__(self, n_refl)
    self.file_name = file_name
    self.n_obs = 0
    self.n_rejected = 0
    self.corr = 0
    self.d_min = -1
    self.accept = False
    self.indexed_cell = None
    self.log_out = file_name
    self.wavelength = None

  def set_indexed_cell (self, unit_cell) :
    self.indexed_cell = unit_cell

  def set_log_out (self, out_str) :
    self.log_out = out_str

  def show_log_out (self, out) :
    print >> out, self.log_out

class null_data (object) :
  """
  Stand-in for a frame rejected due to conflicting symmetry.  (No flex arrays
  included, to save pickling time during multiprocessing.)
  """
  def __init__ (self, file_name, log_out,
                file_error=False,
                low_signal=False,
                wrong_bravais=False,
                wrong_cell=False) :
    adopt_init_args(self, locals())

  def show_log_out (self, out) :
    print >> out, self.log_out

class unit_cell_distribution (object) :
  """
  Container for collecting unit cell edge length statistics - both for frames
  included in the final dataset, and those rejected due to poor correlation.
  (Frames with incompatible indexing solutions will not be included.)
  """
  # TODO make this more general - currently assumes that angles are fixed,
  # which is true for the systems studied so far
  def __init__ (self) :
    self.uc_a_values = flex.double()
    self.uc_b_values = flex.double()
    self.uc_c_values = flex.double()
    self.all_uc_a_values = flex.double()
    self.all_uc_b_values = flex.double()
    self.all_uc_c_values = flex.double()

  def add_cell (self, unit_cell, rejected=False) :
    if (unit_cell is None) :
      return
    (a,b,c,alpha,beta,gamma) = unit_cell.parameters()
    if (not rejected) :
      self.uc_a_values.append(a)
      self.uc_b_values.append(b)
      self.uc_c_values.append(c)
    self.all_uc_a_values.append(a)
    self.all_uc_b_values.append(b)
    self.all_uc_c_values.append(c)

  def add_cells(self, uc) :
    """Addition operation for unit cell statistics."""
    self.uc_a_values.extend(uc.uc_a_values)
    self.uc_b_values.extend(uc.uc_b_values)
    self.uc_c_values.extend(uc.uc_c_values)
    self.all_uc_a_values.extend(uc.all_uc_a_values)
    self.all_uc_b_values.extend(uc.all_uc_b_values)
    self.all_uc_c_values.extend(uc.all_uc_c_values)

  def show_histograms (self, reference, out, n_slots=20) :
    [a0,b0,c0,alpha0,beta0,gamma0] = reference.parameters()
    print >> out, ""
    labels = ["a","b","c"]
    ref_edges = [a0,b0,c0]
    def _show_each (edges) :
      for edge, ref_edge, label in zip(edges, ref_edges, labels) :
        h = flex.histogram(edge, n_slots=n_slots)
        smin, smax = flex.min(edge), flex.max(edge)
        stats = flex.mean_and_variance(edge)
        print >> out, "  %s edge" % label
        print >> out, "     range:     %6.2f - %.2f" % (smin, smax)
        print >> out, "     mean:      %6.2f +/- %6.2f on N = %d" % (
          stats.mean(), stats.unweighted_sample_standard_deviation(), edge.size())
        print >> out, "     reference: %6.2f" % ref_edge
        h.show(f=out, prefix="    ", format_cutoffs="%6.2f")
        print >> out, ""
    edges = [self.all_uc_a_values, self.all_uc_b_values, self.all_uc_c_values]
    print >> out, \
      "Unit cell length distribution (all frames with compatible indexing):"
    _show_each(edges)
    edges = [self.uc_a_values, self.uc_b_values, self.uc_c_values]
    print >> out, \
      "Unit cell length distribution (frames with acceptable correlation):"
    _show_each(edges)

  def get_average_cell_dimensions (self) :
    a = flex.mean(self.uc_a_values)
    b = flex.mean(self.uc_b_values)
    c = flex.mean(self.uc_c_values)
    return a,b,c

#-----------------------------------------------------------------------
class scaling_manager (intensity_data) :
  def __init__ (self, miller_set, i_model, params, log=None) :
    if (log is None) :
      log = sys.stdout
    self.log = log
    self.params = params
    self.miller_set = miller_set
    self.i_model = i_model
    self.ref_bravais_type = bravais_lattice(
      miller_set.space_group_info().type().number())
    intensity_data.__init__(self, miller_set.size())
    self.reverse_lookup = None
    if params.merging.reverse_lookup is not None:
      self.reverse_lookup = easy_pickle.load(params.merging.reverse_lookup)
    self.reset()

  def reset (self) :
    self.n_processed = 0
    self.n_accepted = 0
    self.n_file_error = 0
    self.n_low_signal = 0
    self.n_wrong_bravais = 0
    self.n_wrong_cell = 0
    self.n_low_corr = 0
    self.observations = flex.int()
    self.corr_values = flex.double()
    self.rejected_fractions = flex.double()
    self.uc_values = unit_cell_distribution()
    self.d_min_values = flex.double()
    self.wavelength = flex.double()
    self.initialize()

  def scale_all (self, file_names) :
    t1 = time.time()
    if self.params.backend == 'MySQL':
      from xfel.cxi.merging_database import manager
    elif self.params.backend == 'SQLite':
      from xfel.cxi.merging_database_sqlite3 import manager
    else:
      from xfel.cxi.merging_database_fs import manager

    db_mgr = manager(self.params)
    db_mgr.initialize_db(self.miller_set.indices())

    # Unless the number of requested processes is greater than one,
    # try parallel multiprocessing on a parallel host.  Block until
    # all database commands have been processed.
    nproc = self.params.nproc
    if (nproc is None) or (nproc is Auto):
      nproc = libtbx.introspection.number_of_processors()
    if nproc > 1:
      try :
        import multiprocessing
        self._scale_all_parallel(file_names, db_mgr)
      except ImportError, e :
        print >> self.log, \
          "multiprocessing module not available (requires Python >= 2.6)\n" \
          "will scale frames serially"
        self._scale_all_serial(file_names, db_mgr)
    else:
      self._scale_all_serial(file_names, db_mgr)
    db_mgr.join()

    t2 = time.time()
    print >> self.log, ""
    print >> self.log, "#" * 80
    print >> self.log, "FINISHED MERGING"
    print >> self.log, "  Elapsed time: %.1fs" % (t2 - t1)
    print >> self.log, "  %d of %d integration files were accepted" % (
      self.n_accepted, len(file_names))
    print >> self.log, "  %d rejected due to wrong Bravais group" % \
      self.n_wrong_bravais
    print >> self.log, "  %d rejected for unit cell outliers" % \
      self.n_wrong_cell
    print >> self.log, "  %d rejected for low signal" % \
      self.n_low_signal
    print >> self.log, "  %d rejected due to poor correlation" % \
      self.n_low_corr
    print >> self.log, "  %d rejected for file errors or no reindex matrix" % \
      self.n_file_error
    checksum = self.n_accepted  + self.n_file_error \
               + self.n_low_corr + self.n_low_signal \
               + self.n_wrong_bravais + self.n_wrong_cell
    assert checksum == len(file_names)

  def _scale_all_parallel (self, file_names, db_mgr) :
    import multiprocessing
    import libtbx.introspection

    nproc = self.params.nproc
    if (nproc is None) or (nproc is Auto) :
      nproc = libtbx.introspection.number_of_processors()

    # Input files are supplied to the scaling processes on demand by
    # means of a queue.
    #
    # XXX The input queue may need to either allow non-blocking
    # put():s or run in a separate process to prevent the procedure
    # from blocking here if the list of file paths does not fit into
    # the queue's buffer.
    input_queue = multiprocessing.Manager().JoinableQueue()
    for file_name in file_names:
      input_queue.put(file_name)

    pool = multiprocessing.Pool(processes=nproc)
    # Each process accumulates its own statistics in serial, and the
    # grand total is eventually collected by the main process'
    # _add_all_frames() function.
    for i in xrange(nproc) :
      sm = scaling_manager(self.miller_set, self.i_model, self.params)
      pool.apply_async(
        func=sm,
        args=[input_queue, db_mgr],
        callback=self._add_all_frames)
    pool.close()
    pool.join()

    # Block until the input queue has been emptied.
    input_queue.join()


  def _scale_all_serial (self, file_names, db_mgr) :
    """
    Scale frames sequentially (single-process).  The return value is
    picked up by the callback.
    """
    for file_name in file_names :
      scaled = self.scale_frame(file_name, db_mgr)
      if (scaled is not None) :
        self.add_frame(scaled)
    return (self)

  def add_frame (self, data) :
    """
    Combine the scaled data from a frame with the current overall dataset.
    Also accepts None or null_data objects, when data are unusable but we
    want to record the file as processed.
    """
    self.n_processed += 1
    if (data is None) :
      return None
    #data.show_log_out(self.log)
    #self.log.flush()
    if (isinstance(data, null_data)) :
      if (data.file_error) :
        self.n_file_error += 1
      elif (data.low_signal) :
        self.n_low_signal += 1
      elif (data.wrong_bravais) :
        self.n_wrong_bravais += 1
      elif (data.wrong_cell) :
        self.n_wrong_cell += 1
      return
    if (data.accept) :
      self.n_accepted    += 1
      self.completeness  += data.completeness
      self.summed_N      += data.summed_N
      self.summed_weight += data.summed_weight
      self.summed_wt_I   += data.summed_wt_I
      for index, isigi in data.ISIGI.iteritems() :
        if (index in self.ISIGI):
          self.ISIGI[index] += isigi
        else:
          self.ISIGI[index] = isigi
    else :
      self.n_low_corr += 1
    self.uc_values.add_cell(data.indexed_cell,
      rejected=(not data.accept))
    self.observations.append(data.n_obs)
    if (data.n_obs > 0) :
      frac_rejected = data.n_rejected / data.n_obs
      self.rejected_fractions.append(frac_rejected)
      self.d_min_values.append(data.d_min)
    self.corr_values.append(data.corr)
    self.wavelength.append(data.wavelength)

  def _add_all_frames (self, data) :
    """The _add_all_frames() function collects the statistics accumulated
    in @p data by the individual scaling processes in the process
    pool.  This callback function is run in serial, so it does not
    need a lock.
    """
    self.n_accepted += data.n_accepted
    self.n_file_error += data.n_file_error
    self.n_low_corr += data.n_low_corr
    self.n_low_signal += data.n_low_signal
    self.n_processed += data.n_processed
    self.n_wrong_bravais += data.n_wrong_bravais
    self.n_wrong_cell += data.n_wrong_cell

    for index, isigi in data.ISIGI.iteritems() :
      if (index in self.ISIGI):
        self.ISIGI[index] += isigi
      else:
        self.ISIGI[index] = isigi

    self.completeness += data.completeness
    self.summed_N += data.summed_N
    self.summed_weight += data.summed_weight
    self.summed_wt_I += data.summed_wt_I

    self.corr_values.extend(data.corr_values)
    self.d_min_values.extend(data.d_min_values)
    self.observations.extend(data.observations)
    self.rejected_fractions.extend(data.rejected_fractions)
    self.wavelength.extend(data.wavelength)

    self.uc_values.add_cells(data.uc_values)

  def show_unit_cell_histograms (self) :
    self.uc_values.show_histograms(
      reference=self.miller_set.unit_cell(),
      out=self.log)

  def get_plot_statistics (self) :
    return plot_statistics(
      prefix=self.params.output.prefix,
      unit_cell_statistics=self.uc_values,
      reference_cell=self.miller_set.unit_cell(),
      correlations=self.corr_values,
      min_corr=self.params.min_corr,
      rejected_fractions=self.rejected_fractions,
      frame_d_min=self.d_min_values)

  def get_overall_correlation (self, sum_I) :
    """
    Correlate the averaged intensities to the intensities from the
    reference data set.  XXX The sum_I argument is really a kludge!
    """
    sum_xx = 0
    sum_xy = 0
    sum_yy = 0
    sum_x  = 0
    sum_y  = 0
    N      = 0
    for i in xrange(len(self.summed_N)):
      if (self.summed_N[i] <= 0):
        continue
      I_r       = self.i_model.data()[i]
      I_o       = sum_I[i]/self.summed_N[i]
      N      += 1
      sum_xx += I_r**2
      sum_yy += I_o**2
      sum_xy += I_r * I_o
      sum_x  += I_r
      sum_y  += I_o
    slope = (N * sum_xy - sum_x * sum_y) / (N * sum_xx - sum_x**2)
    corr  = (N * sum_xy - sum_x * sum_y) / (math.sqrt(N * sum_xx - sum_x**2) *
             math.sqrt(N * sum_yy - sum_y**2))
    print >> self.log, \
      "SUMMARY: For %d reflections, got slope %f, correlation %f" \
        % (N, slope, corr)
    return N, corr

  def finalize_and_save_data (self) :
    """
    Assemble a Miller array with the summed data, setting the unit cell to
    the consensus average if desired, and write to an MTZ file (including
    merged/non-anomalous data too).
    """
    print >> self.log, ""
    print >> self.log, "#" * 80
    print >> self.log, "OUTPUT FILES"
    Iobs_all = flex.double(self.miller_set.size())
    SigI_all = flex.double(self.miller_set.size())
    for i in xrange(len(Iobs_all)):
      if (self.summed_weight[i] > 0.):
        Iobs_all[i] = self.summed_wt_I[i] / self.summed_weight[i]
        SigI_all[i] = math.sqrt(1. / self.summed_weight[i])
    if (self.params.set_average_unit_cell) :
      # XXX since XFEL crystallography runs at room temperature, it may not
      # be appropriate to use the cell dimensions from a cryo structure.
      # also, some runs seem to have huge variance in the indexed cell
      # dimensions, so downstream programs (MR, refinement) may run better
      # with the cell set to the mean edge lengths.
      abc = self.uc_values.get_average_cell_dimensions()
      print >> self.log, "  (will use final unit cell edges %g %g %g)" % abc
      angles = self.miller_set.unit_cell().parameters()[3:]
      unit_cell = uctbx.unit_cell(list(abc) + list(angles))
      final_symm = symmetry(
        unit_cell=unit_cell,
        space_group_info=self.miller_set.space_group_info())
    else :
      final_symm = self.miller_set
    all_obs = miller.array(
      miller_set=self.miller_set.customized_copy(
        crystal_symmetry=final_symm),
      data=Iobs_all,
      sigmas=SigI_all).resolution_filter(
        d_min=self.params.d_min).set_observation_type_xray_intensity()
    mtz_file = "%s.mtz" % self.params.output.prefix
    all_obs = all_obs.select(all_obs.data() > 0)
    mtz_out = all_obs.as_mtz_dataset(
      column_root_label="Iobs",
      title=self.params.output.title,
      wavelength=flex.mean(self.wavelength))
    mtz_out.add_miller_array(
      miller_array=all_obs.average_bijvoet_mates(),
      column_root_label="IMEAN")
    mtz_obj = mtz_out.mtz_object()
    mtz_obj.write(mtz_file)
    print >> self.log, "  Anomalous and mean data:\n    %s" % \
      os.path.abspath(mtz_file)
    print >> self.log, ""
    print >> self.log, "Final data:"
    all_obs.show_summary(self.log, prefix="  ")
    return mtz_file, all_obs

  def __call__ (self, input_queue, db_mgr) :
    # Scale frames sequentially within the current process.  The
    # return value is picked up by the callback.  See also
    # self.scale_all_serial()
    from Queue import Empty

    try :
      while True:
        try:
          file_name = input_queue.get_nowait()
        except Empty:
          return self

        scaled = self.scale_frame(file_name, db_mgr)
        if scaled is not None:
          self.add_frame(scaled)
        input_queue.task_done()

    except Exception, e :
      print >> self.log, str(e)
      return None

  def scale_frame (self, file_name, db_mgr) :
    """The scale_frame() function populates a back end database with
    appropriately scaled intensities derived from a single frame.  The
    mark0 scaling algorithm determines the scale factor by correlating
    the frame's corrected intensities to those of a reference
    structure, while the mark1 algorithm applies no scaling at all.
    The scale_frame() function can be called either serially or via a
    multiprocessing map() function.

    @note This function must not modify any internal data or the
          parallelization will not yield usable results!

    @param file_name Path to integration pickle file
    @param db_mgr    Back end database manager
    @return          An intensity_data object
    """

    out = StringIO()
    wrong_cell = wrong_bravais = False
    reindex_op = self.params.data_reindex_op
    if self.reverse_lookup is not None:
      reindex_op = self.reverse_lookup.get(file_name, None)
      if reindex_op is None:
        return null_data(file_name=file_name, log_out=out.getvalue(), file_error=True)
    try :
      result = load_result(
        file_name=file_name,
        reference_cell=self.params.target_unit_cell,
        ref_bravais_type=self.ref_bravais_type,
        params=self.params,
        reindex_op = reindex_op,
        out=out)
      if result is None:
        return null_data(
          file_name=file_name, log_out=out.getvalue(), file_error=True)
    except OutlierCellError, e :
      print >> out, str(e)
      return null_data(
        file_name=file_name, log_out=out.getvalue(), wrong_cell=True)
    except WrongBravaisError, e :
      print >> out, str(e)
      return null_data(
        file_name=file_name, log_out=out.getvalue(), wrong_bravais=True)

    # If the pickled integration file does not contain a wavelength,
    # fall back on the value given on the command line.  XXX The
    # wavelength parameter should probably be removed from master_phil
    # once all pickled integration files contain it.
    if (result.has_key("wavelength")):
      wavelength = result["wavelength"]
    elif (self.params.wavelength is not None):
      wavelength = self.params.wavelength
    else:
      # XXX Give error, or raise exception?
      return None
    assert (wavelength > 0)

    observations = result["observations"][0]
    cos_two_polar_angle = result["cos_two_polar_angle"]

    assert observations.size() == cos_two_polar_angle.size()
    tt_vec = observations.two_theta(wavelength)
    cos_tt_vec = flex.cos( tt_vec.data() )
    sin_tt_vec = flex.sin( tt_vec.data() )
    cos_sq_tt_vec = cos_tt_vec * cos_tt_vec
    sin_sq_tt_vec = sin_tt_vec * sin_tt_vec
    P_nought_vec = 0.5 * (1. + cos_sq_tt_vec)

    F_prime = -1.0 # Hard-coded value defines the incident polarization axis
    P_prime = 0.5 * F_prime * cos_two_polar_angle * sin_sq_tt_vec
    observations = observations / ( P_nought_vec - P_prime )
    # This corrects observations for polarization assuming 100% polarization on
    # one axis (thus the F_prime = -1.0 rather than the perpendicular axis, 1.0)
    # Polarization model as described by Kahn, Fourme, Gadet, Janin, Dumas & Andre
    # (1982) J. Appl. Cryst. 15, 330-337, equations 13 - 15.

    print "Step 3. Correct for polarization."
    indexed_cell = observations.unit_cell()

    observations_original_index = observations.deep_copy()
    if result.get("model_partialities",None) is not None and result["model_partialities"][0] is not None:
      # some recordkeeping useful for simulations
      partialities_original_index = observations.customized_copy(
        crystal_symmetry=self.miller_set.crystal_symmetry(),
        data = result["model_partialities"][0]["data"],
        sigmas = flex.double(result["model_partialities"][0]["data"].size()), #dummy value for sigmas
        indices = result["model_partialities"][0]["indices"],
        ).resolution_filter(d_min=self.params.d_min)

    assert len(observations_original_index.indices()) == len(observations.indices())

    # Now manipulate the data to conform to unit cell, asu, and space group
    # of reference.  The resolution will be cut later.
    # Only works if there is NOT an indexing ambiguity!
    observations = observations.customized_copy(
      anomalous_flag=not self.params.merge_anomalous,
      crystal_symmetry=self.miller_set.crystal_symmetry()
      ).map_to_asu()

    observations_original_index = observations_original_index.customized_copy(
      anomalous_flag=not self.params.merge_anomalous,
      crystal_symmetry=self.miller_set.crystal_symmetry()
      )
    print "Step 4. Filter on global resolution and map to asu"
    print >> out, "Data in reference setting:"
    #observations.show_summary(f=out, prefix="  ")
    show_observations(observations, out=out)

    if self.params.significance_filter.apply is True: #------------------------------------
      # Apply an I/sigma filter ... accept resolution bins only if they
      #   have significant signal; tends to screen out higher resolution observations
      #   if the integration model doesn't quite fit
      N_obs_pre_filter = observations.size()
      N_bins_small_set = N_obs_pre_filter // self.params.significance_filter.min_ct
      N_bins_large_set = N_obs_pre_filter // self.params.significance_filter.max_ct

      # Ensure there is at least one bin.
      N_bins = max(
        [min([self.params.significance_filter.n_bins,N_bins_small_set]),
         N_bins_large_set, 1]
      )
      print "Total obs %d Choose n bins = %d"%(N_obs_pre_filter,N_bins)
      bin_results = show_observations(observations, out=out, n_bins=N_bins)
      show_observations(observations, out=sys.stdout, n_bins=N_bins)
      acceptable_resolution_bins = [
        bin.mean_I_sigI > self.params.significance_filter.sigma for bin in bin_results]
      acceptable_nested_bin_sequences = [i for i in xrange(len(acceptable_resolution_bins))
                                         if False not in acceptable_resolution_bins[:i+1]]
      if len(acceptable_nested_bin_sequences)==0:
        return null_data(
          file_name=file_name, log_out=out.getvalue(), low_signal=True)
      else:
        N_acceptable_bins = max(acceptable_nested_bin_sequences) + 1
        imposed_res_filter = float(bin_results[N_acceptable_bins-1].d_range.split()[2])
        imposed_res_sel = observations.resolution_filter_selection(
          d_min=imposed_res_filter)
        observations = observations.select(
          imposed_res_sel)
        observations_original_index = observations_original_index.select(
          imposed_res_sel)
        print "New resolution filter at %7.2f"%imposed_res_filter,file_name
      print "N acceptable bins",N_acceptable_bins
      print "Old n_obs: %d, new n_obs: %d"%(N_obs_pre_filter,observations.size())
      print "Step 5. Frame by frame resolution filter"
      # Finished applying the binwise I/sigma filter---------------------------------------

    if self.params.raw_data.sdfac_auto is True:
      I_over_sig = observations.data()/observations.sigmas()
      #assert that at least a few I/sigmas are less than zero
      Nlt0 = I_over_sig.select(I_over_sig<0.).size()
      if Nlt0 > 2:
        # get a rough estimate for the SDFAC, assuming that negative measurements
        # represent false predictions and therefore normally distributed noise.
        no_signal = I_over_sig.select(I_over_sig<0.)
        for xns in xrange(len(no_signal)):
          no_signal.append(-no_signal[xns])
        Stats = flex.mean_and_variance(no_signal)
        SDFAC = Stats.unweighted_sample_standard_deviation()
      else: SDFAC=1.
      print "The applied SDFAC is %7.4f"%SDFAC
      corrected_sigmas = observations.sigmas() * SDFAC
      observations = observations.customized_copy(sigmas = corrected_sigmas)

    print "Step 6.  Match to reference intensities, filter by correlation, filter out negative intensities."
    assert len(observations_original_index.indices()) \
      ==   len(observations.indices())

    data = frame_data(self.n_refl, file_name)
    data.set_indexed_cell(indexed_cell)
    data.d_min = observations.d_min()

    # Ensure that match_multi_indices() will return identical results
    # when a frame's observations are matched against the
    # pre-generated Miller set, self.miller_set, and the reference
    # data set, self.i_model.  The implication is that the same match
    # can be used to map Miller indices to array indices for intensity
    # accumulation, and for determination of the correlation
    # coefficient in the presence of a scaling reference.
    if self.i_model is not None:
      assert len(self.i_model.indices()) == len(self.miller_set.indices()) \
        and  (self.i_model.indices() ==
              self.miller_set.indices()).count(False) == 0

    matches = miller.match_multi_indices(
      miller_indices_unique=self.miller_set.indices(),
      miller_indices=observations.indices())

    use_weights = False # New facility for getting variance-weighted correlation
    if self.params.scaling.algorithm == 'mark1':
      # Because no correlation is computed, the correlation
      # coefficient is fixed at zero.  Setting slope = 1 means
      # intensities are added without applying a scale factor.
      sum_x = 0
      sum_y = 0
      for pair in matches.pairs():
        data.n_obs += 1
        if observations.data()[pair[1]] <= 0:
          data.n_rejected += 1
        else:
          sum_y += observations.data()[pair[1]]
      N = data.n_obs - data.n_rejected

      slope = 1
      offset = 0
      corr = 0

    else:
      sum_xx = 0
      sum_xy = 0
      sum_yy = 0
      sum_x = 0
      sum_y = 0
      sum_w = 0.

      for pair in matches.pairs():
        if self.params.scaling.simulation is not None:
          observations.data()[pair[1]] = self.i_model.data()[pair[0]]     # SIM
          observations.sigmas()[pair[1]] = self.i_model.sigmas()[pair[0]] # SIM

        data.n_obs += 1
        if observations.data()[pair[1]] <= 0:
          data.n_rejected += 1
          continue
        # Update statistics using reference intensities (I_r), and
        # observed intensities (I_o).
        if use_weights: I_w = 1./(observations.sigmas()[pair[1]])**2 #variance weighting
        else: I_w = 1.

        I_r = self.i_model.data()[pair[0]]
        I_o = observations.data()[pair[1]]
        sum_xx += I_w * I_r**2
        sum_yy += I_w * I_o**2
        sum_xy += I_w * I_r * I_o
        sum_x += I_w * I_r
        sum_y += I_w * I_o
        sum_w += I_w
      # Linearly fit I_r to I_o, i.e. find slope and offset such that
      # I_o = slope * I_r + offset, optimal in a least-squares sense.
      # XXX This is backwards, really.
      N = data.n_obs - data.n_rejected
      DELTA = sum_w * sum_xx - sum_x**2 # see p. 105 in Bevington & Robinson
      if (DELTA) == 0:
        print "Skipping frame with",sum_w,sum_xx,sum_x**2
        return null_data(file_name=file_name,
                         log_out=out.getvalue(),
                         low_signal=True)
      slope = (sum_w * sum_xy - sum_x * sum_y) / DELTA
      offset = (sum_xx * sum_y - sum_x * sum_xy) / DELTA
      corr = (sum_w * sum_xy - sum_x * sum_y) / (math.sqrt(sum_w * sum_xx - sum_x**2) *
                                                 math.sqrt(sum_w * sum_yy - sum_y**2))

    # Early return if there are no positive reflections on the frame.
    if data.n_obs <= data.n_rejected:
      return null_data(
        file_name=file_name, log_out=out.getvalue(), low_signal=True)

    # Update the count for each matched reflection.  This counts
    # reflections with non-positive intensities, too.
    data.completeness += matches.number_of_matches(0).as_int()
    data.corr = corr
    data.wavelength = wavelength

    if self.params.postrefinement.enable: #New code prototype postrefinement; assumes mosaicity=0 & monochromatic
      from scitbx import lbfgs
      from libtbx import adopt_init_args

      print "Old correlation is", corr
      pair1 = flex.int([pair[1] for pair in matches.pairs()])
      pair0 = flex.int([pair[0] for pair in matches.pairs()])
      #raw_input("go:")
      # narrow things down to the set that matches, only
      observations = observations.customized_copy(
        indices = flex.miller_index([observations.indices()[p] for p in pair1]),
        data = flex.double([observations.data()[p] for p in pair1]),
        sigmas = flex.double([observations.sigmas()[p] for p in pair1]),
      )
      observations_original_index = observations_original_index.customized_copy(
        indices = flex.miller_index([observations_original_index.indices()[p] for p in pair1]),
        data = flex.double([observations_original_index.data()[p] for p in pair1]),
        sigmas = flex.double([observations_original_index.sigmas()[p] for p in pair1]),
      )

      #IOBSVEC = flex.double([observations.data()[p] for p in pair1])
      #MILLER = flex.miller_index([observations_original_index.indices()[p] for p in pair1])
      IOBSVEC = observations.data()
      ICALCVEC = flex.double([self.i_model.data()[p] for p in pair0])
      MILLER = observations_original_index.indices()
      print "ZZZ",observations.size(), observations_original_index.size(), len(MILLER)
      ORI = result["current_orientation"][0]
      Astar = matrix.sqr(ORI.reciprocal_matrix())
      WAVE = result["wavelength"]
      BEAM = matrix.col((0.0,0.0,-1./WAVE))
      BFACTOR = 0.
      DSSQ = ORI.unit_cell().d_star_sq(MILLER)

      if self.params.postrefinement.algorithm=="rs":
        Rhall = flex.double()
        for mill in MILLER:
          H = matrix.col(mill)
          Xhkl = Astar*H
          Rh = ( Xhkl + BEAM ).length() - (1./WAVE)
          Rhall.append(Rh)
        Rs = math.sqrt(flex.mean(Rhall*Rhall))

        RS = 1./10000. # reciprocal effective domain size of 1 micron
        RS = Rs        # try this empirically determined approximate, monochrome, a-mosaic value
        current = flex.double([slope, BFACTOR, RS, 0., 0.])

        class unpack(object):
         def __init__(YY,values):
          YY.reference = values # simply the flex double list of parameters
         def __getattr__(YY,item):
          if item=="thetax" : return YY.reference[3]
          if item=="thetay" : return YY.reference[4]
          if item=="G" :      return YY.reference[0]
          if item=="BFACTOR": return YY.reference[1]
          if item=="RS":      return YY.reference[2]
          return getattr(YY,item)

         def show(values):
          print "%10.7f"%values.G,
          print "%10.7f"%values.BFACTOR, \
                "%10.7f"%values.RS, \
                "%7.3f deg %7.3f deg"%(
            180.*values.thetax/math.pi,180.*values.thetay/math.pi)

        def lorentz_callable(values):
          return get_partiality_array(values)

        def get_partiality_array(values):
          rs = values.RS
          Rh = get_Rh_array(values)
          rs_sq = rs*rs
          PB = rs_sq / ((2. * (Rh * Rh)) + rs_sq)
          return PB

      elif self.params.postrefinement.algorithm=="eta_deff":
        DVEC = ORI.unit_cell().d(MILLER)
        eta_init = 2. * result["ML_half_mosaicity_deg"][0] * math.pi/180.
        D_eff_init = 2.*result["ML_domain_size_ang"][0]
        current = flex.double([slope, BFACTOR, eta_init, 0., 0.,D_eff_init,])

        class unpack(object):
         def __init__(YY,values):
          YY.reference = values # simply the flex double list of parameters
         def __getattr__(YY,item):
          if item=="thetax" : return YY.reference[3]
          if item=="thetay" : return YY.reference[4]
          if item=="G" :      return YY.reference[0]
          if item=="BFACTOR": return YY.reference[1]
          if item=="ETA":      return YY.reference[2]
          if item=="DEFF":      return YY.reference[5]
          return getattr(YY,item)

         def show(values):
          print "%10.7f"%values.G,
          print "%10.7f"%values.BFACTOR, \
                "eta %10.7f"%values.ETA, \
                "Deff %10.2f"%values.DEFF, \
                "%7.3f deg %7.3f deg"%(
            180.*values.thetax/math.pi,180.*values.thetay/math.pi)

        def lorentz_callable(values):
          Rh = get_Rh_array(values)
          Rs = flex.double(len(MILLER),1./values.DEFF)+flex.double(len(MILLER),values.ETA/2.)/DVEC
          ratio = Rh / Rs
          ratio_abs = flex.abs(ratio)
          return ratio_abs

        def get_partiality_array(values):
          Rh = get_Rh_array(values)
          Rs = flex.double(len(MILLER),1./values.DEFF)+flex.double(len(MILLER),values.ETA/2.)/DVEC
          Rs_sq = Rs * Rs
          Rh_sq = Rh * Rh
          numerator = Rs_sq - Rh_sq
          denominator = values.DEFF * Rs * Rs_sq
          partiality = numerator / denominator
          return partiality

      def get_Rh_array(values):
        Rh = flex.double()
        eff_Astar = get_eff_Astar(values)
        for mill in MILLER:
          x = eff_Astar * matrix.col(mill)
          Svec = x + BEAM
          Rh.append(Svec.length() - (1./WAVE))
        return Rh

      def get_eff_Astar(values):
        thetax = values.thetax; thetay = values.thetay;
        effective_orientation = ORI.rotate_thru((1,0,0),thetax
           ).rotate_thru((0,1,0),thetay
           )
        return matrix.sqr(effective_orientation.reciprocal_matrix())

      def scaler_callable(values):
        PB = get_partiality_array(values)
        EXP = flex.exp(-2.*values.BFACTOR*DSSQ)
        terms = values.G * EXP * PB
        return terms

      def fvec_callable(values):
        PB = get_partiality_array(values)
        EXP = flex.exp(-2.*values.BFACTOR*DSSQ)
        terms = (values.G * EXP * PB * ICALCVEC - IOBSVEC)
        # Ideas for improvement
        #   straightforward to also include sigma weighting
        #   add extra terms representing rotational excursion: terms.concatenate(1.e7*Rh)
        return terms

      func = fvec_callable(unpack(current))
      functional = flex.sum(func*func)
      print "functional",functional

      class c_minimizer:

        def __init__(self, current_x=None,
                     min_iterations=0, max_calls=1000, max_drop_eps=1.e-5):
          adopt_init_args(self, locals())
          self.n = current_x.size()
          self.x = current_x
          self.minimizer = lbfgs.run(
            target_evaluator=self,
            termination_params=lbfgs.termination_parameters(
              traditional_convergence_test=False,
              drop_convergence_test_max_drop_eps=max_drop_eps,
              min_iterations=min_iterations,
              max_iterations = None,
              max_calls=max_calls),
            exception_handling_params=lbfgs.exception_handling_parameters(
               ignore_line_search_failed_rounding_errors=True,
               ignore_line_search_failed_step_at_lower_bound=True,#the only change from default
               ignore_line_search_failed_step_at_upper_bound=False,
               ignore_line_search_failed_maxfev=False,
               ignore_line_search_failed_xtol=False,
               ignore_search_direction_not_descent=False)
            )

        def compute_functional_and_gradients(self):
          values = unpack(self.x)
          assert -150. < values.BFACTOR < 150. # limits on the exponent, please
          self.func = fvec_callable(values)
          functional = flex.sum(self.func*self.func)
          self.f = functional
          DELTA = 1.E-7
          self.g = flex.double()
          for x in xrange(self.n):
            templist = list(self.x)
            templist[x]+=DELTA
            dvalues = flex.double(templist)

            dfunc = fvec_callable(unpack(dvalues))
            dfunctional = flex.sum(dfunc*dfunc)
            #calculate by finite_difference
            self.g.append( ( dfunctional-functional )/DELTA )
          self.g[2]=0.
          print "rms %10.3f"%math.sqrt(flex.mean(self.func*self.func)),
          values.show()
          return self.f, self.g

        def __del__(self):
          values = unpack(self.x)
          print "FINALMODEL",
          print "rms %10.3f"%math.sqrt(flex.mean(self.func*self.func)),
          values.show()

      try:
        MINI = c_minimizer( current_x = current )
      except AssertionError: # on exponential overflow
        return null_data(
               file_name=file_name, log_out=out.getvalue(), low_signal=True)
      scaler = scaler_callable(unpack(MINI.x))
      if self.params.postrefinement.algorithm=="rs":
        fat_selection = (lorentz_callable(unpack(MINI.x)) > 0.2)
      else:
        fat_selection = (lorentz_callable(unpack(MINI.x)) < 0.9)
      fat_count = fat_selection.count(True)

      #avoid empty database INSERT, if there are insufficient centrally-located Bragg spots:
      if fat_count < 3:
        return null_data(
               file_name=file_name, log_out=out.getvalue(), low_signal=True)
      print "On total %5d the fat selection is %5d"%(len(observations.indices()), fat_count)
      print "ZZZ",observations.size(), observations_original_index.size(), len(fat_selection), len(scaler)
      observations_original_index = observations_original_index.select(fat_selection)

      observations = observations.customized_copy(
        indices = observations.indices().select(fat_selection),
        data = (observations.data()/scaler).select(fat_selection),
        sigmas = (observations.sigmas()/scaler).select(fat_selection)
      )
      matches = miller.match_multi_indices(
        miller_indices_unique=self.miller_set.indices(),
        miller_indices=observations.indices())

    if not self.params.scaling.enable or self.params.postrefinement.enable: # Do not scale anything
      print "Scale factor to an isomorphous reference PDB will NOT be applied."
      slope = 1.0
      offset = 0.0

    print result.get("sa_parameters")[0]
    have_sa_params = ( type(result.get("sa_parameters")[0]) == type(dict()) )
    #have_sa_params = (result.get("sa_parameters")[0].find('None')!=0)
    observations_original_index_indices = observations_original_index.indices()

    #cell_params = data.indexed_cell.parameters()
    #reserve_cell_params = result["sa_parameters"][0]["reserve_orientation"].unit_cell().parameters()
    # cell params and reserve cell params are essentially equal within numerical precision

    kwargs = {'wavelength': wavelength,
              'beam_x': result['xbeam'],
              'beam_y': result['ybeam'],
              'distance': result['distance'],
              'c_c': corr,
              'slope': slope,
              'offset': offset,
              'unique_file_name': data.file_name}

    if have_sa_params:
      sa_parameters = result['sa_parameters'][0]
      res_ori_direct = sa_parameters['reserve_orientation'].direct_matrix().elems

      kwargs['res_ori_1'] = res_ori_direct[0]
      kwargs['res_ori_2'] = res_ori_direct[1]
      kwargs['res_ori_3'] = res_ori_direct[2]
      kwargs['res_ori_4'] = res_ori_direct[3]
      kwargs['res_ori_5'] = res_ori_direct[4]
      kwargs['res_ori_6'] = res_ori_direct[5]
      kwargs['res_ori_7'] = res_ori_direct[6]
      kwargs['res_ori_8'] = res_ori_direct[7]
      kwargs['res_ori_9'] = res_ori_direct[8]

      kwargs['rotation100_rad'] = sa_parameters.rotation100_rad
      kwargs['rotation010_rad'] = sa_parameters.rotation010_rad
      kwargs['rotation001_rad'] = sa_parameters.rotation001_rad

      kwargs['half_mosaicity_deg'] = sa_parameters.half_mosaicity_deg
      kwargs['wave_HE_ang'] = sa_parameters.wave_HE_ang
      kwargs['wave_LE_ang'] = sa_parameters.wave_LE_ang
      kwargs['domain_size_ang'] = sa_parameters.domain_size_ang

    else:
      res_ori_direct = matrix.sqr(
        data.indexed_cell.orthogonalization_matrix()).transpose().elems

      kwargs['res_ori_1'] = res_ori_direct[0]
      kwargs['res_ori_2'] = res_ori_direct[1]
      kwargs['res_ori_3'] = res_ori_direct[2]
      kwargs['res_ori_4'] = res_ori_direct[3]
      kwargs['res_ori_5'] = res_ori_direct[4]
      kwargs['res_ori_6'] = res_ori_direct[5]
      kwargs['res_ori_7'] = res_ori_direct[6]
      kwargs['res_ori_8'] = res_ori_direct[7]
      kwargs['res_ori_9'] = res_ori_direct[8]
      if self.params.scaling.report_ML:
        kwargs['half_mosaicity_deg'] = result["ML_half_mosaicity_deg"][0]
        kwargs['domain_size_ang'] = result["ML_domain_size_ang"][0]
      else:
        kwargs['half_mosaicity_deg'] =float("NaN")
        kwargs['domain_size_ang'] =float("NaN")

    frame_id_0_base = db_mgr.insert_frame(**kwargs)

    xypred = result["mapped_predictions"][0]
    indices = flex.size_t([pair[1] for pair in matches.pairs()])

    sel_observations = flex.intersection(
      size=observations.data().size(),
      iselections=[indices])
    set_original_hkl = observations_original_index_indices.select(
      flex.intersection(
        size=observations_original_index_indices.size(),
        iselections=[indices]))
    set_xypred = xypred.select(
      flex.intersection(
        size=xypred.size(),
        iselections=[indices]))

    kwargs = {'hkl_id_0_base': [pair[0] for pair in matches.pairs()],
              'i': observations.data().select(sel_observations),
              'sigi': observations.sigmas().select(sel_observations),
              'detector_x': [xy[0] for xy in set_xypred],
              'detector_y': [xy[1] for xy in set_xypred],
              'frame_id_0_base': [frame_id_0_base] * len(matches.pairs()),
              'overload_flag': [0] * len(matches.pairs()),
              'original_h': [hkl[0] for hkl in set_original_hkl],
              'original_k': [hkl[1] for hkl in set_original_hkl],
              'original_l': [hkl[2] for hkl in set_original_hkl]}

    db_mgr.insert_observation(**kwargs)

    if False:
      # ******************************************************
      # try a new procedure to scale obs to the reference data with K & B,
      # to minimize (for positive Iobs only) functional...
      # ahead of doing this, section simply plots calc & obs...
      # ******************************************************
      print "For %d reflections, got slope %f, correlation %f" % \
        (data.n_obs - data.n_rejected, slope, corr)
      print "average obs", sum_y / (data.n_obs - data.n_rejected), \
        "average calc", sum_x / (data.n_obs - data.n_rejected), \
        "offset",offset
      print "Rejected %d reflections with negative intensities" % \
        data.n_rejected

      reference= flex.double()
      observed=flex.double()
      for pair in matches.pairs():
        if (observations.data()[pair[1]] -offset <= 0):
          continue
        I_r = self.i_model.data()[pair[0]]
        I_o = observations.data()[pair[1]]
        reference.append(I_r)
        observed.append((I_o - offset)/slope)

      from matplotlib import pyplot as plt
      plt.plot(flex.log10(observed),flex.log10(reference),"r.")
      plt.show()

    print >> out, "For %d reflections, got slope %f, correlation %f" % \
        (data.n_obs - data.n_rejected, slope, corr)
    print >> out, "average obs", sum_y / (data.n_obs - data.n_rejected), \
      "average calc", sum_x / (data.n_obs - data.n_rejected)
    print >> out, "Rejected %d reflections with negative intensities" % \
        data.n_rejected

    # Apply the correlation coefficient threshold, if appropriate.
    if self.params.scaling.algorithm == 'mark0' and \
       corr <= self.params.min_corr:
      print >> out, "Skipping these data - correlation too low."
    else:
      data.accept = True
      for pair in matches.pairs():
        if (observations.data()[pair[1]] <= 0) :
          continue
        Intensity = observations.data()[pair[1]] / slope

        # Add the reflection as a two-tuple of intensity and I/sig(I)
        # to the dictionary of observations.
        index = self.miller_set.indices()[pair[0]]
        isigi = (Intensity,
                 observations.data()[pair[1]] / observations.sigmas()[pair[1]])
        if index in data.ISIGI:
          data.ISIGI[index].append(isigi)
        else:
          data.ISIGI[index] = [isigi]

        sigma = observations.sigmas()[pair[1]] / slope
        variance = sigma * sigma
        data.summed_N[pair[0]] += 1
        data.summed_wt_I[pair[0]] += Intensity / variance
        data.summed_weight[pair[0]] += 1 / variance
    data.set_log_out(out.getvalue())
    if corr > 0.5:
      print "Selected file %s"%file_name.replace("integration","out").replace("int","idx")
      print "Selected distance %6.2f mm"%float(result["distance"])
      data.show_log_out(sys.stdout)
    return data

#-----------------------------------------------------------------------
def run(args):
  phil = iotbx.phil.process_command_line(args=args, master_string=master_phil).show()
  work_params = phil.work.extract()
  from xfel.cxi.merging.phil_validation import application
  application(work_params)
  if ("--help" in args) :
    libtbx.phil.parse(master_phil.show())
    return

  if ((work_params.d_min is None) or
      (work_params.data is None) or
      ( (work_params.model is None) and work_params.scaling.algorithm != "mark1") ) :
    command_name = os.environ["LIBTBX_DISPATCHER_NAME"]
    raise Usage(command_name + " "
                "d_min=4.0 "
                "data=~/scratch/r0220/006/strong/ "
                "model=3bz1_3bz2_core.pdb")
  if ((work_params.rescale_with_average_cell) and
      (not work_params.set_average_unit_cell)) :
    raise Usage("If rescale_with_average_cell=True, you must also specify "+
      "set_average_unit_cell=True.")
  # Read Nat's reference model from an MTZ file.  XXX The observation
  # type is given as F, not I--should they be squared?  Check with Nat!
  log = open("%s.log" % work_params.output.prefix, "w")
  out = multi_out()
  out.register("log", log, atexit_send_to=None)
  out.register("stdout", sys.stdout)
  print >> out, "I model"
  if work_params.model is not None:
    from xfel.cxi.merging.general_fcalc import run
    i_model = run(work_params)
    work_params.target_unit_cell = i_model.unit_cell()
    work_params.target_space_group = i_model.space_group_info()
    i_model.show_summary()
  else:
    i_model = None

  print >> out, "Target unit cell and space group:"
  print >> out, "  ", work_params.target_unit_cell
  print >> out, "  ", work_params.target_space_group

  # Adjust the minimum d-spacing of the generated Miller set to assure
  # that the desired high-resolution limit is included even if the
  # observed unit cell differs slightly from the target.  If a
  # reference model is present, ensure that Miller indices are ordered
  # identically.
  miller_set = symmetry(
      unit_cell=work_params.target_unit_cell,
      space_group_info=work_params.target_space_group
    ).build_miller_set(
      anomalous_flag=not work_params.merge_anomalous,
      d_max=work_params.d_max,
      d_min=work_params.d_min / math.pow(
        1 + work_params.unit_cell_length_tolerance, 1 / 3))
  miller_set = miller_set.change_basis(
    work_params.model_reindex_op).map_to_asu()

  if i_model is not None:
    matches = miller.match_indices(i_model.indices(), miller_set.indices())
    assert not matches.have_singles()
    miller_set = miller_set.select(matches.permutation())

  frame_files = get_observations(work_params.data, work_params.data_subset)
  scaler = scaling_manager(
    miller_set=miller_set,
    i_model=i_model,
    params=work_params,
    log=out)
  scaler.scale_all(frame_files)
  if scaler.n_accepted == 0:
    return None
  scaler.show_unit_cell_histograms()
  if (work_params.rescale_with_average_cell) :
    average_cell_abc = scaler.uc_values.get_average_cell_dimensions()
    average_cell = uctbx.unit_cell(list(average_cell_abc) +
      list(work_params.target_unit_cell.parameters()[3:]))
    work_params.target_unit_cell = average_cell
    print >> out, ""
    print >> out, "#" * 80
    print >> out, "RESCALING WITH NEW TARGET CELL"
    print >> out, "  average cell: %g %g %g %g %g %g" % \
      work_params.target_unit_cell.parameters()
    print >> out, ""
    scaler.reset()
    scaler.scale_all(frame_files)
    scaler.show_unit_cell_histograms()
  if False : #(work_params.output.show_plots) :
    try :
      plot_overall_completeness(completeness)
    except Exception, e :
      print "ERROR: can't show plots"
      print "  %s" % str(e)
  print >> out, "\n"

  # Sum the observations of I and I/sig(I) for each reflection.
  sum_I = flex.double(miller_set.size(), 0.)
  sum_I_SIGI = flex.double(miller_set.size(), 0.)
  for i in xrange(miller_set.size()) :
    index = miller_set.indices()[i]
    if index in scaler.ISIGI :
      for t in scaler.ISIGI[index]:
        sum_I[i] += t[0]
        sum_I_SIGI[i] += t[1]

  miller_set_avg = miller_set.customized_copy(
    unit_cell=work_params.target_unit_cell)
  table1 = show_overall_observations(
    obs=miller_set_avg,
    redundancy=scaler.completeness,
    summed_wt_I=scaler.summed_wt_I,
    summed_weight=scaler.summed_weight,
    ISIGI=scaler.ISIGI,
    n_bins=work_params.output.n_bins,
    title="Statistics for all reflections",
    out=out,
    work_params=work_params)
  print >> out, ""
  if work_params.model is not None:
    n_refl, corr = scaler.get_overall_correlation(sum_I)
  else:
    n_refl, corr = ((scaler.completeness > 0).count(True), 0)
  print >> out, "\n"
  table2 = show_overall_observations(
    obs=miller_set_avg,
    redundancy=scaler.summed_N,
    summed_wt_I=scaler.summed_wt_I,
    summed_weight=scaler.summed_weight,
    ISIGI=scaler.ISIGI,
    n_bins=work_params.output.n_bins,
    title="Statistics for reflections where I > 0",
    out=out,
    work_params=work_params)
  #from libtbx import easy_pickle
  #easy_pickle.dump(file_name="stats.pickle", obj=stats)
  #stats.report(plot=work_params.plot)
  #miller_counts = miller_set_p1.array(data=stats.counts.as_double()).select(
  #  stats.counts != 0)
  #miller_counts.as_mtz_dataset(column_root_label="NOBS").mtz_object().write(
  #  file_name="nobs.mtz")
  print >> out, ""
  mtz_file, miller_array = scaler.finalize_and_save_data()
  #table_pickle_file = "%s_graphs.pkl" % work_params.output.prefix
  #easy_pickle.dump(table_pickle_file, [table1, table2])
  loggraph_file = os.path.abspath("%s_graphs.log" % work_params.output.prefix)
  f = open(loggraph_file, "w")
  f.write(table1.format_loggraph())
  f.write("\n")
  f.write(table2.format_loggraph())
  f.close()
  result = scaling_result(
    miller_array=miller_array,
    plots=scaler.get_plot_statistics(),
    mtz_file=mtz_file,
    loggraph_file=loggraph_file,
    obs_table=table1,
    all_obs_table=table2,
    n_reflections=n_refl,
    overall_correlation=corr)
  easy_pickle.dump("%s.pkl" % work_params.output.prefix, result)
  return result

def show_overall_observations(
  obs, redundancy, summed_wt_I, summed_weight, ISIGI, n_bins=15, out=None, title=None, work_params=None):
  if out is None:
    out = sys.stdout
  obs.setup_binner(d_max=100000, d_min=work_params.d_min, n_bins=n_bins)
  result = []

  cumulative_unique = 0
  cumulative_meas   = 0
  cumulative_theor  = 0
  cumulative_In     = 0
  cumulative_I      = 0.0
  cumulative_Isigma = 0.0

  for i_bin in obs.binner().range_used():
    sel_w = obs.binner().selection(i_bin)
    sel_fo_all = obs.select(sel_w)
    d_range = obs.binner().bin_legend(
      i_bin=i_bin, show_bin_number=False, show_counts=False)
    sel_redundancy = redundancy.select(sel_w)
    sel_absent = sel_redundancy.count(0)
    n_present = sel_redundancy.size() - sel_absent
    sel_complete_tag = "[%d/%d]" % (n_present, sel_redundancy.size())
    sel_measurements = flex.sum(sel_redundancy)

    # Alternatively, redundancy (or multiplicity) is calculated as the
    # average number of observations for the observed
    # reflections--missing reflections do not affect the redundancy
    # adversely, and the reported value becomes
    # completeness-independent.
    val_redundancy_obs = 0
    if n_present > 0:
      val_redundancy_obs = flex.sum(sel_redundancy) / n_present

    # Per-bin sum of I and I/sig(I).  For any reflection, the weight
    # of the merged intensity must be positive for this to make sense.
    sel_o = (sel_w & (summed_weight > 0))
    intensity = summed_wt_I.select(sel_o) / summed_weight.select(sel_o)
    I_sum = flex.sum(intensity)
    I_sigI_sum = flex.sum(intensity * flex.sqrt(summed_weight.select(sel_o)))
    I_n = sel_o.count(True)

    # Per-bin sum of I and I/sig(I) for each observation.
    # R-merge statistics have been removed because
    #  >> R-merge is defined on whole structure factor intensities, either
    #     full observations or summed partials from the rotation method.
    #     For XFEL data all reflections are assumed to be partial; no
    #     method exists now to convert partials to fulls.
    if work_params.plot_single_index_histograms: import numpy as np
    for i in obs.binner().array_indices(i_bin) :
      index = obs.indices()[i]
      if (index in ISIGI) :
        # Compute m, the "merged" intensity, as the average intensity
        # of all observations of the reflection with the given index.
        N = 0
        m = 0
        for t in ISIGI[index] :
          N += 1
          m += t[0]
        if work_params is not None and \
           (work_params.plot_single_index_histograms and \
            N >= 30 and \
            work_params.data_subset == 0):
          print "Miller %20s n-obs=%4d  sum-I=%10.0f"%(index, N, m)
          plot_n_bins = N//10
          hist,bins = np.histogram([t[0] for t in ISIGI[index]],bins=25)
          width = 0.7*(bins[1]-bins[0])
          center = (bins[:-1]+bins[1:])/2
          import matplotlib.pyplot as plt
          plt.bar(center, hist, align="center", width=width)
          plt.show()

    if sel_measurements > 0:
      mean_I = mean_I_sigI = 0
      if I_n > 0:
        mean_I = I_sum / I_n
        mean_I_sigI = I_sigI_sum / I_n
      bin = resolution_bin(
        i_bin=i_bin,
        d_range=d_range,
        d_min=obs.binner().bin_d_min(i_bin),
        redundancy_asu=flex.mean(sel_redundancy.as_double()),
        redundancy_obs=val_redundancy_obs,
        complete_tag=sel_complete_tag,
        completeness=n_present / sel_redundancy.size(),
        measurements=sel_measurements,
        mean_I=mean_I,
        mean_I_sigI=mean_I_sigI)
      result.append(bin)
    cumulative_unique += n_present
    cumulative_meas   += sel_measurements
    cumulative_theor  += sel_redundancy.size()
    cumulative_In     += I_n
    cumulative_I      += I_sum
    cumulative_Isigma += I_sigI_sum

  if (title is not None) :
    print >> out, title
  from libtbx import table_utils
  table_header = ["","","","<asu","<obs","","","",""]
  table_header2 = ["Bin","Resolution Range","Completeness","redun>","redun>","n_meas","<I>","<I/sig(I)>"]
  table_data = []
  table_data.append(table_header)
  table_data.append(table_header2)
  for bin in result:
    table_row = []
    table_row.append("%3d" % bin.i_bin)
    table_row.append("%-13s" % bin.d_range)
    table_row.append("%13s" % bin.complete_tag)
    table_row.append("%6.2f" % bin.redundancy_asu)
    table_row.append("%6.2f" % bin.redundancy_obs)
    table_row.append("%6d" % bin.measurements)
    table_row.append("%8.0f" % bin.mean_I)
    table_row.append("%8.3f" % bin.mean_I_sigI)
    table_data.append(table_row)
  table_data.append([""]*len(table_header))
  table_data.append(  [
      format_value("%3s",   "All"),
      format_value("%-13s", "                 "),
      format_value("%13s",  "[%d/%d]"%(cumulative_unique,cumulative_theor)),
      format_value("%6.2f", cumulative_meas/cumulative_theor),
      format_value("%6.2f", cumulative_meas/cumulative_unique),
      format_value("%6d",   cumulative_meas),
      format_value("%8.0f", cumulative_I/cumulative_In),
      format_value("%8.3f", cumulative_Isigma/cumulative_In),
  ])

  print
  print >>out,table_utils.format(table_data,has_header=2,justify='center',delim=" ")

  # XXX generate table object for displaying plots
  if (title is None) :
    title = "Data statistics by resolution"
  table = data_plots.table_data(
    title=title,
    x_is_inverse_d_min=True,
    force_exact_x_labels=True)
  table.add_column(
    column=[1 / bin.d_min**2 for bin in result],
    column_name="d_min",
    column_label="Resolution")
  table.add_column(
    column=[bin.redundancy_asu for bin in result],
    column_name="redundancy",
    column_label="Redundancy")
  table.add_column(
    column=[bin.completeness for bin in result],
    column_name="completeness",
    column_label="Completeness")
  table.add_column(
    column=[bin.mean_I_sigI for bin in result],
    column_name="mean_i_over_sigI",
    column_label="<I/sig(I)>")
  table.add_graph(
    name="Redundancy vs. resolution",
    type="GRAPH",
    columns=[0,1])
  table.add_graph(
    name="Completeness vs. resolution",
    type="GRAPH",
    columns=[0,2])
  table.add_graph(
    name="<I/sig(I)> vs. resolution",
    type="GRAPH",
    columns=[0,3])
  return table

class resolution_bin(object):
  def __init__(self,
               i_bin=None,
               d_range=None,
               d_min=None,
               redundancy_asu=None,
               redundancy_obs=None,
               absent=None,
               complete_tag=None,
               completeness=None,
               measurements=None,
               mean_I=None,
               mean_I_sigI=None,
               sigmaa=None):
    adopt_init_args(self, locals())

class scaling_result (group_args) :
  """
  Container for any objects that might need to be saved for future use (e.g.
  in a GUI).  Must be pickle-able!
  """
  pass

#-----------------------------------------------------------------------
# graphical goodies
def plot_overall_completeness(completeness):
  completeness_range = xrange(-1,flex.max(completeness)+1)
  completeness_counts = [completeness.count(n) for n in completeness_range]
  from matplotlib import pyplot as plt
  plt.plot(completeness_range,completeness_counts,"r+")
  plt.show()

class plot_statistics (object) :
  """
  Container for assorted histograms of frame statistics.  The resolution bin
  plots are stored separately, since they can be displayed using the loggraph
  viewer.
  """
  def __init__ (self,
                prefix,
                unit_cell_statistics,
                reference_cell,
                correlations,
                min_corr,
                rejected_fractions,
                frame_d_min) :
    adopt_init_args(self, locals())

  def show_all_pyplot (self, n_slots=20) :
    """
    Display histograms using pyplot.  For use in a wxPython GUI the figure
    should be created separately in a wx.Frame.
    """
    from matplotlib import pyplot as plt
    fig = plt.figure(figsize=(9,12))
    self.plot_unit_cell_histograms(
      figure=fig,
      a_values=self.unit_cell_statistics.all_uc_a_values,
      b_values=self.unit_cell_statistics.all_uc_b_values,
      c_values=self.unit_cell_statistics.all_uc_c_values,
      n_slots=n_slots,
      title=\
        "Unit cell length distribution (all frames with compatible indexing): %s" % self.prefix)
    plt.show()
    fig = plt.figure(figsize=(9,12))
    self.plot_unit_cell_histograms(
      figure=fig,
      a_values=self.unit_cell_statistics.uc_a_values,
      b_values=self.unit_cell_statistics.uc_b_values,
      c_values=self.unit_cell_statistics.uc_c_values,
      n_slots=n_slots,
      title=\
        "Unit cell length distribution (frames with acceptable correlation): %s" % self.prefix)
    plt.show()
    fig = plt.figure(figsize=(9,12))
    self.plot_statistics_histograms(
      figure=fig,
      n_slots=n_slots)

  def plot_unit_cell_histograms (self,
      figure,
      a_values,
      b_values,
      c_values,
      n_slots=20,
      title="Distribution of unit cell edge lengths") :
    [a0,b0,c0,alpha0,beta0,gamma0] = self.reference_cell.parameters()
    ax1 = figure.add_axes([0.1, 0.1, 0.8, 0.25])
    ax2 = figure.add_axes([0.1, 0.4, 0.8, 0.25])
    ax3 = figure.add_axes([0.1, 0.7, 0.8, 0.25])
    ax1.hist(c_values, n_slots, color=[1.0,0.0,0.0])
    ax2.hist(b_values, n_slots, color=[0.0,1.0,0.0])
    ax3.hist(a_values, n_slots, color=[0.0,0.5,1.0])
    ax1.axvline(c0, linestyle='.', linewidth=2, color='k')
    ax2.axvline(b0, linestyle='.', linewidth=2, color='k')
    ax3.axvline(a0, linestyle='.', linewidth=2, color='k')
    ax1.set_xlabel("c edge")
    ax2.set_xlabel("b edge")
    ax3.set_xlabel("a edge")
    ax3.set_title("%s: %s" % (title, self.prefix))

  def plot_statistics_histograms (self,
      figure,
      n_slots=20) :
    ax1 = figure.add_axes([0.1, 0.1, 0.8, 0.25])
    ax2 = figure.add_axes([0.1, 0.4, 0.8, 0.25])
    ax3 = figure.add_axes([0.1, 0.7, 0.8, 0.25])
    ax1.hist(self.correlations, n_slots, color=[1.0,0.0,0.0])
    ax2.hist(self.rejected_fractions, n_slots, color=[0.0,1.0,0.0])
    ax3.hist(self.d_min_values, n_slots, color=[0.0,0.5,1.0])
    ax1.axvline(self.min_corr, linestyle='.', linewidth=2, color='k')
    ax1.set_xlabel("Correlation to reference dataset")
    ax2.set_xlabel("Fraction of rejected zero or negative intensities")
    ax3.set_xlabel("Integrated resolution limit")
    ax1.set_title("Correlation by frame (%s)" % self.prefix)
    ax2.set_title("Rejected reflections by frame (%s)" % self.prefix)
    ax3.set_title("Resolution by frame (%s)" % self.prefix)
    plt.show()

if (__name__ == "__main__"):
  show_plots = False
  if ("--plots" in sys.argv) :
    sys.argv.remove("--plots")
    show_plots = True
  result = run(args=sys.argv[1:])
  if result is None:
    sys.exit(1)
  if (show_plots) :
    try :
      result.plots.show_all_pyplot()
      from wxtbx.command_line import loggraph
      loggraph.run([result.loggraph_file])
    except Exception, e :
      print "Can't display plots"
      print "You should be able to view them by running this command:"
      print "  wxtbx.loggraph %s" % result.loggraph_file
      raise e
